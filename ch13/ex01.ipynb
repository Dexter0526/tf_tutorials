{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 글자 레벨 기계 번역기(Character-Level Neural Machine Translation) 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다운로드 링크 : http://www.manythings.org/anki  \n",
    "한국 코퍼스 데이터의 양이 작으므로 프랑스-영어 병렬 코퍼스인 fra-eng.zip 파일을 사용권장. 위의 링크에서 해당 파일을 다운받으시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 병렬 코퍼스 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv('C:/Users/it/Downloads/dataset/kor-eng/kor.txt', names=['src', 'tar', 'att'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "      <th>att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who?</td>\n",
       "      <td>누구?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>안녕!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No way!</td>\n",
       "      <td>절대 아니야.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No way!</td>\n",
       "      <td>그럴리가!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Goodbye!</td>\n",
       "      <td>안녕!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm sad.</td>\n",
       "      <td>슬퍼.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Me, too.</td>\n",
       "      <td>나도.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Perfect!</td>\n",
       "      <td>완벽해!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shut up!</td>\n",
       "      <td>시끄러워!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Welcome.</td>\n",
       "      <td>어서오세요.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        src      tar                                                att\n",
       "0      Who?      누구?  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1    Hello!      안녕!  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
       "2   No way!  절대 아니야.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3   No way!    그럴리가!  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "4  Goodbye!      안녕!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "5  I'm sad.      슬퍼.  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
       "6  Me, too.      나도.  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
       "7  Perfect!     완벽해!  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
       "8  Shut up!    시끄러워!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
       "9  Welcome.   어서오세요.  CC-BY 2.0 (France) Attribution: tatoeba.org #1..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lines[\"att\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>I hate myself sometimes.</td>\n",
       "      <td>난 가끔 내 자신이 싫어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Certain religions are against organ donation.</td>\n",
       "      <td>어떤 종교는 장기 기증을 금지하기도 해.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>They're amateurs.</td>\n",
       "      <td>걔네 초짜야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>I love my home.</td>\n",
       "      <td>난 내 집이 좋아.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>I'd like to reserve a table for two.</td>\n",
       "      <td>두 명 자리를 예약하고 싶어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>When does your winter vacation begin?</td>\n",
       "      <td>겨울 방학은 언제 시작하나요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Ebola spreads from person to person through bo...</td>\n",
       "      <td>에볼라는 체액을 통하여 사람에서 사람으로 전파된다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Everybody here except me has done that.</td>\n",
       "      <td>나 빼고 여기에 있는 사람 모두 그것을 했다. (한 적이 있다.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Tom went to bed very late.</td>\n",
       "      <td>톰은 엄청 늦게 잤어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>You'd better not tell him.</td>\n",
       "      <td>그에게 말하지 않는게 좋을걸.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   src  \\\n",
       "360                           I hate myself sometimes.   \n",
       "822      Certain religions are against organ donation.   \n",
       "154                                  They're amateurs.   \n",
       "86                                     I love my home.   \n",
       "708               I'd like to reserve a table for two.   \n",
       "731              When does your winter vacation begin?   \n",
       "882  Ebola spreads from person to person through bo...   \n",
       "748            Everybody here except me has done that.   \n",
       "449                         Tom went to bed very late.   \n",
       "455                         You'd better not tell him.   \n",
       "\n",
       "                                      tar  \n",
       "360                        난 가끔 내 자신이 싫어.  \n",
       "822                어떤 종교는 장기 기증을 금지하기도 해.  \n",
       "154                               걔네 초짜야.  \n",
       "86                             난 내 집이 좋아.  \n",
       "708                     두 명 자리를 예약하고 싶어요.  \n",
       "731                      겨울 방학은 언제 시작하나요?  \n",
       "882          에볼라는 체액을 통하여 사람에서 사람으로 전파된다.  \n",
       "748  나 빼고 여기에 있는 사람 모두 그것을 했다. (한 적이 있다.)  \n",
       "449                          톰은 엄청 늦게 잤어.  \n",
       "455                      그에게 말하지 않는게 좋을걸.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Tom's birthday was the day before yesterday.</td>\n",
       "      <td>\\t 톰의 생일은 그저께였다. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>It's popular in Australia.</td>\n",
       "      <td>\\t 호주에서는 인기가 있어요. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>I took a walk with my dog.</td>\n",
       "      <td>\\t 내 개와 산책했어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>I have money for you.</td>\n",
       "      <td>\\t 너에게 줄 돈이 있다. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>He was at the bottom of the class.</td>\n",
       "      <td>\\t 걔는 반에서 꼴찌였어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>You will know soon enough.</td>\n",
       "      <td>\\t 곧 충분히 알게 될거야. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>Tom has high blood pressure.</td>\n",
       "      <td>\\t 톰은 고혈압이다. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>Are you Swedish? \"No, I'm Swiss.\"</td>\n",
       "      <td>\\t 스웨덴 사람이세요? \"아니요, 스위스 사람이예요.\" \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>Tom and his brother look quite similar.</td>\n",
       "      <td>\\t 톰이랑 톰네 형은 진짜 닮았어. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>I have to introduce Tom to the manager.</td>\n",
       "      <td>\\t 나는 톰을 매니저에게 소개시켜줘야 합니다. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              src  \\\n",
       "820  Tom's birthday was the day before yesterday.   \n",
       "438                    It's popular in Australia.   \n",
       "436                    I took a walk with my dog.   \n",
       "266                         I have money for you.   \n",
       "659            He was at the bottom of the class.   \n",
       "454                    You will know soon enough.   \n",
       "515                  Tom has high blood pressure.   \n",
       "685             Are you Swedish? \"No, I'm Swiss.\"   \n",
       "758       Tom and his brother look quite similar.   \n",
       "752       I have to introduce Tom to the manager.   \n",
       "\n",
       "                                    tar  \n",
       "820                 \\t 톰의 생일은 그저께였다. \\n  \n",
       "438                \\t 호주에서는 인기가 있어요. \\n  \n",
       "436                    \\t 내 개와 산책했어. \\n  \n",
       "266                  \\t 너에게 줄 돈이 있다. \\n  \n",
       "659                  \\t 걔는 반에서 꼴찌였어. \\n  \n",
       "454                 \\t 곧 충분히 알게 될거야. \\n  \n",
       "515                     \\t 톰은 고혈압이다. \\n  \n",
       "685  \\t 스웨덴 사람이세요? \"아니요, 스위스 사람이예요.\" \\n  \n",
       "758             \\t 톰이랑 톰네 형은 진짜 닮았어. \\n  \n",
       "752       \\t 나는 톰을 매니저에게 소개시켜줘야 합니다. \\n  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "662\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'а']\n",
      "['개', '걀', '걔', '거', '걱', '건', '걸', '검', '겁', '것', '게', '겠', '겨', '격', '견', '결', '겼', '경', '계', '고', '곤', '곧', '곱', '곳', '공', '과', '관', '괜', '괴', '교']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '8': 15, '9': 16, ':': 17, ';': 18, '?': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'R': 36, 'S': 37, 'T': 38, 'U': 39, 'V': 40, 'W': 41, 'Y': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68, 'а': 69}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '(': 6, ')': 7, ',': 8, '-': 9, '.': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '8': 18, '9': 19, '?': 20, 'A': 21, 'B': 22, 'H': 23, 'M': 24, 'T': 25, 'a': 26, 'd': 27, 'h': 28, 'i': 29, 'm': 30, 'o': 31, 'p': 32, 'r': 33, 't': 34, 'y': 35, '가': 36, '각': 37, '간': 38, '갈': 39, '감': 40, '갑': 41, '값': 42, '갔': 43, '강': 44, '같': 45, '개': 46, '걀': 47, '걔': 48, '거': 49, '걱': 50, '건': 51, '걸': 52, '검': 53, '겁': 54, '것': 55, '게': 56, '겠': 57, '겨': 58, '격': 59, '견': 60, '결': 61, '겼': 62, '경': 63, '계': 64, '고': 65, '곤': 66, '곧': 67, '곱': 68, '곳': 69, '공': 70, '과': 71, '관': 72, '괜': 73, '괴': 74, '교': 75, '구': 76, '국': 77, '군': 78, '굳': 79, '궁': 80, '권': 81, '귀': 82, '그': 83, '극': 84, '근': 85, '글': 86, '금': 87, '급': 88, '기': 89, '긴': 90, '길': 91, '까': 92, '깎': 93, '깐': 94, '깔': 95, '깨': 96, '꺼': 97, '꺾': 98, '껍': 99, '껏': 100, '께': 101, '꼈': 102, '꼴': 103, '꽃': 104, '꽉': 105, '꽤': 106, '꾸': 107, '꿨': 108, '뀌': 109, '끄': 110, '끈': 111, '끊': 112, '끌': 113, '끓': 114, '끔': 115, '끗': 116, '끝': 117, '끼': 118, '나': 119, '난': 120, '날': 121, '남': 122, '납': 123, '났': 124, '내': 125, '낼': 126, '냄': 127, '냈': 128, '냉': 129, '냐': 130, '냥': 131, '너': 132, '넌': 133, '네': 134, '녀': 135, '녁': 136, '년': 137, '념': 138, '녕': 139, '노': 140, '논': 141, '놀': 142, '농': 143, '놓': 144, '놔': 145, '놨': 146, '누': 147, '눈': 148, '뉴': 149, '느': 150, '는': 151, '늘': 152, '늙': 153, '능': 154, '늦': 155, '니': 156, '님': 157, '다': 158, '닥': 159, '단': 160, '닫': 161, '달': 162, '닮': 163, '담': 164, '답': 165, '당': 166, '대': 167, '댔': 168, '더': 169, '덕': 170, '던': 171, '데': 172, '덴': 173, '도': 174, '독': 175, '돈': 176, '돌': 177, '돕': 178, '동': 179, '돼': 180, '됐': 181, '되': 182, '된': 183, '될': 184, '됩': 185, '두': 186, '둔': 187, '둘': 188, '둬': 189, '뒀': 190, '뒤': 191, '드': 192, '든': 193, '듣': 194, '들': 195, '듯': 196, '등': 197, '디': 198, '딨': 199, '따': 200, '딸': 201, '땋': 202, '때': 203, '떠': 204, '떤': 205, '떨': 206, '떴': 207, '떻': 208, '뜨': 209, '뜻': 210, '라': 211, '락': 212, '란': 213, '랄': 214, '람': 215, '랍': 216, '랐': 217, '랑': 218, '래': 219, '랜': 220, '랩': 221, '러': 222, '럭': 223, '런': 224, '럴': 225, '럼': 226, '럽': 227, '렀': 228, '렇': 229, '레': 230, '렌': 231, '려': 232, '렵': 233, '렸': 234, '례': 235, '로': 236, '록': 237, '론': 238, '롭': 239, '료': 240, '루': 241, '류': 242, '륙': 243, '르': 244, '른': 245, '를': 246, '름': 247, '릅': 248, '리': 249, '린': 250, '릴': 251, '림': 252, '마': 253, '막': 254, '만': 255, '많': 256, '말': 257, '맙': 258, '맛': 259, '망': 260, '맞': 261, '맡': 262, '매': 263, '머': 264, '먹': 265, '멍': 266, '메': 267, '멕': 268, '며': 269, '면': 270, '명': 271, '몇': 272, '모': 273, '목': 274, '몰': 275, '못': 276, '무': 277, '묶': 278, '문': 279, '묻': 280, '물': 281, '뭐': 282, '뭔': 283, '뭘': 284, '미': 285, '믿': 286, '밀': 287, '바': 288, '박': 289, '밖': 290, '반': 291, '받': 292, '발': 293, '밟': 294, '밤': 295, '방': 296, '배': 297, '백': 298, '뱀': 299, '버': 300, '번': 301, '벌': 302, '법': 303, '벗': 304, '베': 305, '벼': 306, '벽': 307, '변': 308, '별': 309, '병': 310, '보': 311, '복': 312, '본': 313, '볼': 314, '봄': 315, '봅': 316, '봉': 317, '봐': 318, '봤': 319, '부': 320, '북': 321, '분': 322, '불': 323, '붉': 324, '붐': 325, '브': 326, '비': 327, '빈': 328, '빌': 329, '빛': 330, '빠': 331, '빨': 332, '빴': 333, '빵': 334, '빼': 335, '뺄': 336, '뻔': 337, '뿐': 338, '사': 339, '산': 340, '살': 341, '삼': 342, '샀': 343, '상': 344, '새': 345, '색': 346, '생': 347, '서': 348, '석': 349, '선': 350, '설': 351, '섯': 352, '성': 353, '세': 354, '셔': 355, '셨': 356, '소': 357, '속': 358, '손': 359, '솔': 360, '송': 361, '쇠': 362, '수': 363, '숙': 364, '순': 365, '술': 366, '숨': 367, '쉽': 368, '슈': 369, '스': 370, '슨': 371, '슬': 372, '습': 373, '시': 374, '식': 375, '신': 376, '실': 377, '싫': 378, '심': 379, '십': 380, '싶': 381, '싸': 382, '쌉': 383, '써': 384, '썩': 385, '썼': 386, '썽': 387, '쓰': 388, '쓸': 389, '씀': 390, '씨': 391, '씬': 392, '씻': 393, '아': 394, '안': 395, '앉': 396, '않': 397, '알': 398, '압': 399, '았': 400, '앙': 401, '앞': 402, '애': 403, '액': 404, '야': 405, '약': 406, '양': 407, '얘': 408, '어': 409, '억': 410, '언': 411, '얼': 412, '엄': 413, '업': 414, '없': 415, '엇': 416, '었': 417, '에': 418, '엔': 419, '여': 420, '역': 421, '연': 422, '열': 423, '염': 424, '였': 425, '영': 426, '옆': 427, '예': 428, '옛': 429, '오': 430, '옥': 431, '온': 432, '올': 433, '옳': 434, '옷': 435, '와': 436, '완': 437, '왔': 438, '왜': 439, '외': 440, '요': 441, '용': 442, '우': 443, '운': 444, '울': 445, '움': 446, '웃': 447, '워': 448, '원': 449, '월': 450, '웠': 451, '웨': 452, '위': 453, '윗': 454, '유': 455, '육': 456, '으': 457, '은': 458, '을': 459, '음': 460, '읍': 461, '응': 462, '의': 463, '이': 464, '익': 465, '인': 466, '일': 467, '읽': 468, '잃': 469, '임': 470, '입': 471, '있': 472, '잊': 473, '자': 474, '작': 475, '잔': 476, '잖': 477, '잘': 478, '잠': 479, '잡': 480, '잤': 481, '장': 482, '재': 483, '쟁': 484, '저': 485, '적': 486, '전': 487, '절': 488, '점': 489, '접': 490, '정': 491, '제': 492, '져': 493, '졌': 494, '조': 495, '족': 496, '존': 497, '졸': 498, '좀': 499, '종': 500, '좋': 501, '죄': 502, '죠': 503, '주': 504, '죽': 505, '준': 506, '줄': 507, '중': 508, '줘': 509, '줬': 510, '즐': 511, '즘': 512, '증': 513, '지': 514, '직': 515, '진': 516, '질': 517, '집': 518, '짓': 519, '짖': 520, '짜': 521, '짝': 522, '쪼': 523, '쪽': 524, '쫓': 525, '쯤': 526, '찌': 527, '찍': 528, '찢': 529, '차': 530, '착': 531, '찮': 532, '참': 533, '찼': 534, '창': 535, '찾': 536, '채': 537, '책': 538, '챘': 539, '처': 540, '척': 541, '천': 542, '철': 543, '청': 544, '체': 545, '쳐': 546, '쳤': 547, '초': 548, '총': 549, '최': 550, '추': 551, '축': 552, '출': 553, '충': 554, '취': 555, '츠': 556, '치': 557, '칙': 558, '친': 559, '침': 560, '카': 561, '캐': 562, '커': 563, '컴': 564, '컵': 565, '켓': 566, '켜': 567, '켰': 568, '코': 569, '쾅': 570, '퀴': 571, '크': 572, '큰': 573, '큼': 574, '키': 575, '타': 576, '탁': 577, '탈': 578, '탐': 579, '탓': 580, '탔': 581, '탕': 582, '태': 583, '택': 584, '터': 585, '턱': 586, '턴': 587, '테': 588, '텐': 589, '토': 590, '톰': 591, '통': 592, '퇴': 593, '투': 594, '트': 595, '틀': 596, '티': 597, '파': 598, '판': 599, '팔': 600, '패': 601, '퍼': 602, '페': 603, '펜': 604, '편': 605, '평': 606, '포': 607, '폭': 608, '폰': 609, '표': 610, '푹': 611, '풀': 612, '퓨': 613, '프': 614, '피': 615, '필': 616, '하': 617, '학': 618, '한': 619, '할': 620, '함': 621, '합': 622, '항': 623, '해': 624, '했': 625, '행': 626, '향': 627, '햿': 628, '허': 629, '헌': 630, '험': 631, '혀': 632, '현': 633, '혈': 634, '혐': 635, '혔': 636, '형': 637, '혜': 638, '호': 639, '혼': 640, '홉': 641, '화': 642, '확': 643, '환': 644, '활': 645, '황': 646, '회': 647, '획': 648, '효': 649, '후': 650, '훔': 651, '훨': 652, '휘': 653, '휴': 654, '흔': 655, '흙': 656, '흡': 657, '흥': 658, '희': 659, '히': 660, '힘': 661}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41, 50, 57, 19], [27, 47, 54, 54, 57, 2], [33, 57, 1, 65, 43, 67, 2], [33, 57, 1, 65, 43, 67, 2], [26, 57, 57, 46, 44, 67, 47, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 147, 76, 20, 3, 2], [1, 3, 395, 139, 4, 3, 2], [1, 3, 488, 167, 3, 394, 156, 405, 10, 3, 2], [1, 3, 83, 225, 249, 36, 4, 3, 2], [1, 3, 395, 139, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 147, 76, 20, 3, 2], [3, 395, 139, 4, 3, 2], [3, 488, 167, 3, 394, 156, 405, 10, 3, 2], [3, 83, 225, 249, 36, 4, 3, 2], [3, 395, 139, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) seq2seq 기계 번역기 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 723 samples, validate on 181 samples\n",
      "Epoch 1/50\n",
      "723/723 [==============================] - 20s 28ms/sample - loss: 2.9279 - val_loss: 2.7356\n",
      "Epoch 2/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.4308 - val_loss: 2.5778\n",
      "Epoch 3/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.3924 - val_loss: 2.5688\n",
      "Epoch 4/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.3852 - val_loss: 2.9253\n",
      "Epoch 5/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.3625 - val_loss: 2.5718\n",
      "Epoch 6/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.3466 - val_loss: 2.7968\n",
      "Epoch 7/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.3396 - val_loss: 3.1174\n",
      "Epoch 8/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.3179 - val_loss: 2.8919\n",
      "Epoch 9/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 1.3047 - val_loss: 2.7352\n",
      "Epoch 10/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.2651 - val_loss: 2.3519\n",
      "Epoch 11/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 1.2631 - val_loss: 2.8079\n",
      "Epoch 12/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 1.2325 - val_loss: 2.8616\n",
      "Epoch 13/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.2132 - val_loss: 2.6343\n",
      "Epoch 14/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.1801 - val_loss: 2.3746\n",
      "Epoch 15/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.1660 - val_loss: 2.4244\n",
      "Epoch 16/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.1133 - val_loss: 2.3301\n",
      "Epoch 17/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 1.0646 - val_loss: 2.1685\n",
      "Epoch 18/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 1.0772 - val_loss: 2.1272\n",
      "Epoch 19/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 1.0116 - val_loss: 2.2131\n",
      "Epoch 20/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 1.0020 - val_loss: 2.0103\n",
      "Epoch 21/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.9903 - val_loss: 2.0025\n",
      "Epoch 22/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.9574 - val_loss: 2.0477\n",
      "Epoch 23/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.9499 - val_loss: 2.0145\n",
      "Epoch 24/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 0.9336 - val_loss: 1.9404\n",
      "Epoch 25/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.9794 - val_loss: 2.0934\n",
      "Epoch 26/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.9093 - val_loss: 1.9615\n",
      "Epoch 27/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 0.8932 - val_loss: 1.9753\n",
      "Epoch 28/50\n",
      "723/723 [==============================] - 16s 21ms/sample - loss: 0.8880 - val_loss: 1.9055\n",
      "Epoch 29/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.8759 - val_loss: 1.9437\n",
      "Epoch 30/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.8664 - val_loss: 1.8452\n",
      "Epoch 31/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.8552 - val_loss: 1.8348\n",
      "Epoch 32/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 0.8462 - val_loss: 1.8436\n",
      "Epoch 33/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.8319 - val_loss: 1.9122\n",
      "Epoch 34/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.8253 - val_loss: 1.8410\n",
      "Epoch 35/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.8108 - val_loss: 1.8441\n",
      "Epoch 36/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.8050 - val_loss: 1.8736\n",
      "Epoch 37/50\n",
      "723/723 [==============================] - 16s 22ms/sample - loss: 0.7933 - val_loss: 1.8566\n",
      "Epoch 38/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.7864 - val_loss: 1.8835\n",
      "Epoch 39/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.7766 - val_loss: 1.8091\n",
      "Epoch 40/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.7651 - val_loss: 1.9014\n",
      "Epoch 41/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 0.7589 - val_loss: 1.8072\n",
      "Epoch 42/50\n",
      "723/723 [==============================] - 15s 21ms/sample - loss: 0.8723 - val_loss: 1.8431\n",
      "Epoch 43/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.7344 - val_loss: 1.8470\n",
      "Epoch 44/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.7263 - val_loss: 1.8108\n",
      "Epoch 45/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.7186 - val_loss: 1.8041\n",
      "Epoch 46/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.7138 - val_loss: 1.8052\n",
      "Epoch 47/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.7065 - val_loss: 1.9291\n",
      "Epoch 48/50\n",
      "723/723 [==============================] - 14s 20ms/sample - loss: 0.7040 - val_loss: 1.8038\n",
      "Epoch 49/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.6931 - val_loss: 1.9590\n",
      "Epoch 50/50\n",
      "723/723 [==============================] - 15s 20ms/sample - loss: 0.6875 - val_loss: 1.8064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c9ffb20148>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체적인 번역 동작 단계를 정리하면 아래와 같습니다.\n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻습니다.\n",
    "2. 상태와 <SOS>에 해당하는 '\\t'를 디코더로 보냅니다.\n",
    "3. 디코더가 <EOS>에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태를 이전 상태로 사용\n",
    "decoder_states = [state_h, state_c]\n",
    "# 이번에는 훈련 과정에서와 달리 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict(\n",
    "    (i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict(\n",
    "    (i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: #stop_condition이 True가 될 때까지 루프 반복\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <sos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: No way!\n",
      "정답 문장:  그럴리가! \n",
      "번역기가 번역한 문장:  톰은 내 가 가 있어. \n",
      "-----------------------------------\n",
      "입력 문장: Welcome.\n",
      "정답 문장:  환영합니다. \n",
      "번역기가 번역한 문장:  톰은 내 가 가 있어. \n",
      "-----------------------------------\n",
      "입력 문장: I'm sorry.\n",
      "정답 문장:  죄송합니다. \n",
      "번역기가 번역한 문장:  톰은 그 가 가 있어. \n",
      "-----------------------------------\n",
      "입력 문장: I felt bad.\n",
      "정답 문장:  난 기분이 나빴다. \n",
      "번역기가 번역한 문장:  톰은 그 가 가 있어. \n",
      "-----------------------------------\n",
      "입력 문장: Do you like rap?\n",
      "정답 문장:  랩 좋아해? \n",
      "번역기가 번역한 문장:  톰은 내 가 가 있어. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,10,20,30,100]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protf",
   "language": "python",
   "name": "protf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
